{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9CYNcJOmmtgi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spikingjelly\n","  Downloading spikingjelly-0.0.0.0.14-py3-none-any.whl (437 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from spikingjelly) (3.7.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from spikingjelly) (1.13.1+cu116)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from spikingjelly) (1.22.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from spikingjelly) (0.14.1+cu116)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from spikingjelly) (4.65.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from spikingjelly) (1.10.1)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (8.4.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (1.4.4)\n","Requirement already satisfied: importlib-resources\u003e=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (5.12.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (3.0.9)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (23.0)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (1.0.7)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (2.8.2)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (4.39.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib-\u003espikingjelly) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch-\u003espikingjelly) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision-\u003espikingjelly) (2.27.1)\n","Requirement already satisfied: zipp\u003e=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources\u003e=3.2.0-\u003ematplotlib-\u003espikingjelly) (3.15.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib-\u003espikingjelly) (1.16.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etorchvision-\u003espikingjelly) (2.0.12)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etorchvision-\u003espikingjelly) (2022.12.7)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etorchvision-\u003espikingjelly) (3.4)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-\u003etorchvision-\u003espikingjelly) (1.26.15)\n","Installing collected packages: spikingjelly\n","Successfully installed spikingjelly-0.0.0.0.14\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf\u003c4,\u003e=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (3.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (1.22.4)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6\n"]}],"source":["!pip install spikingjelly\n","!pip install tensorboardX\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWgwa4zdDa4G"},"outputs":[],"source":["# from google.colab import files\n","# uploaded = files.upload()\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","import sys\n","sys.path.insert(0, 'drive/MyDrive/Colab Notebooks/CS 679 Project')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UjyD13j4S0H"},"outputs":[],"source":["import time\n","import logging\n","from data_builder import *\n","import argparse\n","from networks_for_CIFAR import *\n","from networks_for_ImageNet import *\n","from utils import *\n","from layers import *\n","from tensorboardX import SummaryWriter\n","from torch.cuda import amp\n","from schedulers import *\n","from Regularization import *\n","import random\n","import os\n","import numpy as np\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","from tqdm.autonotebook import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcKLaNvMzaKL"},"outputs":[],"source":["####################################################\n","# args                                             #\n","#                                                  #\n","####################################################\n","class Args:\n","    def __init__(self, eval=False, eval_resume='./raw/models', train_resume='./raw/models', batch_size=16, epochs=3, \n","                 learning_rate=1e-1, momentum=0.9, weight_decay=4e-5, seed=9, auto_continue=False, display_interval=10, \n","                 save_interval=10, dataset_path='./dataset/', train_dir='./fmnist/train', val_dir='./fmnist/val', \n","                 tunable_lif=True, amp=False, modeltag='SNN', gate=[0.6, 0.8, 0.6], static_gate=False, static_param=False, \n","                 channel_wise=False, softsimple=False, soft_mode=False, t=3, randomgate=False, fmnist=False, celeba=True):\n","        self.eval = eval\n","        self.eval_resume = eval_resume\n","        self.train_resume = train_resume\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.learning_rate = learning_rate\n","        self.momentum = momentum\n","        self.weight_decay = weight_decay\n","        self.seed = seed\n","        self.auto_continue = auto_continue\n","        self.display_interval = display_interval\n","        self.save_interval = save_interval\n","        self.dataset_path = dataset_path\n","        self.train_dir = train_dir\n","        self.val_dir = val_dir\n","        self.tunable_lif = tunable_lif\n","        self.amp = amp\n","        self.modeltag = modeltag\n","        self.gate = gate\n","        self.static_gate = static_gate\n","        self.static_param = static_param\n","        self.channel_wise = channel_wise\n","        self.softsimple = softsimple\n","        self.soft_mode = soft_mode\n","        self.t = t\n","        self.randomgate = randomgate\n","        self.fmnist = fmnist\n","        self.celeba = celeba\n"]},{"cell_type":"markdown","metadata":{"id":"ndZKanaCklBV"},"source":["# Data Builder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfrRT2eFUGlJ"},"outputs":[],"source":["def seed_all(seed=1):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","args = Args()\n","\n","seed_all(args.seed)\n","\n","if torch.cuda.device_count() \u003e 1:\n","    local_rank = int(os.environ[\"LOCAL_RANK\"])\n","    torch.distributed.init_process_group(backend=\"nccl\")\n","    torch.cuda.set_device(local_rank)\n","\n","# Log\n","log_format = '[%(asctime)s] %(message)s'\n","logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n","    format=log_format, datefmt='%d %I:%M:%S')\n","t = time.time()\n","local_time = time.localtime(t)\n","if not os.path.exists('./log'):\n","    os.mkdir('./log')\n","fh = logging.FileHandler(os.path.join('log/train-{}{:02}{}'.format(local_time.tm_year % 2000, local_time.tm_mon, t)))\n","fh.setFormatter(logging.Formatter(log_format))\n","logging.getLogger().addHandler(fh)\n","\n","epochs = 1\n","initial_dict = {'gate': [0.6, 0.8, 0.6], 'param': [tau, Vth, linear_decay, conduct],\n","                't': steps, 'static_gate': True, 'static_param': False, 'time_wise': True, 'soft_mode': False}\n","initial_dict['gate'] = args.gate\n","initial_dict['static_gate'] = args.static_gate\n","initial_dict['static_param'] = args.static_param\n","initial_dict['time_wise'] = False\n","initial_dict['soft_mode'] = args.soft_mode\n","if args.t != steps:\n","    initial_dict['t']=args.t\n","\n","# In case time step is too large, we intuitively recommend to use the following code to alleviate the linear decay\n","# initial_dict['param'][2] = initial_dict['param'][1]/(initial_dict['t'] * 2)\n","\n","\n","use_gpu = False\n","if torch.cuda.is_available():\n","    use_gpu = True\n","if args.fmnist:\n","    train_loader, val_loader, _ = build_data(dpath=args.dataset_path,dataset='FashionMNIST',\n","                                              batch_size=args.batch_size, train_val_split=False, workers=2)\n","else:\n","    train_loader, val_loader, _ = build_data(dpath=args.dataset_path,dataset='CelebA',\n","                                              batch_size=args.batch_size, train_val_split=False, workers=2)\n","\n","print('load data successfully')\n","\n","print(initial_dict)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nGGSmPfwrCxf"},"source":["# Train \u0026 Test "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1680188093569,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"wnusWEodUADi"},"outputs":[],"source":["log_interval = 200\n","\n","def train(args, model, device, train_loader, optimizer, epoch, writer, scaler=None):\n","    layer_cnt, gate_score_list = None, None\n","    t1 = time.time()\n","    train_loss = 0\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n","        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n","        optimizer.zero_grad()\n","        if scaler is not None:\n","            with amp.autocast():\n","                output, input, mean, log_variances = model(data)\n","                train_loss = model.loss_function(mean, log_variances, output, input)\n","                total_loss = train_loss['loss']\n","\n","        else:\n","            output, input, mean, log_variances = model(data)\n","            train_loss = model.loss_function(means=mean, log_variances=log_variances, output=output, target=input)\n","            total_loss = train_loss['loss']\n","\n","        if scaler is not None:\n","            scaler.scale(total_loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","        else:\n","            total_loss.backward()\n","            optimizer.step()\n","\n","        # if batch_idx % log_interval == 0:\n","        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n","        #         100. * batch_idx / len(train_loader),\n","        #         total_loss.item() / len(data)))\n","    # average_train_loss = train_loss / len(train_loader.dataset)\n","    return train_loss\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680188093569,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"Nt7y6TNTUGfK"},"outputs":[],"source":["def test(args, model, device, test_loader, epoch, writer, modeltag, dict_params, best= None):\n","    layer_cnt, gate_score_list = None, None\n","    test_loss = 0\n","    model.eval()# inactivate BN\n","    t1 = time.time()\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n","            output, input, mean, log_variances = model(data)\n","            total_loss, recon_loss, kld_loss = model.loss_function(mean, log_variances, output, input)\n","            test_loss+=total_loss\n","\n","        record_param(args, model, dict=dict_params, epoch=epoch, modeltag=modeltag)\n","    average_test_loss = test_loss / len(test_loader.dataset)\n","    print('====\u003e Test set loss: {:.4f}'.format(average_test_loss))\n","    return average_test_loss\n"]},{"cell_type":"markdown","metadata":{"id":"JDMKt6eNnog6"},"source":["# Model\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680188093569,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"zApegZpfnl44"},"outputs":[],"source":["\n","class VanillaVAE_GLIF(nn.Module):\n","    def __init__(self, lif_param:dict, tunable_lif=False, in_channels=3, hidden_dims=[32, 64, 128, 256, 512], \n","                latent_dim=128,\n","                beta: int = 4,\n","                gamma:float = 10.,\n","                max_capacity: int = 25,\n","                Capacity_max_iter: int = 1e5,\n","                loss_type = \"beta\",\n","                kld_weight_corrector = 1.0):\n","        super(VanillaVAE_GLIF, self).__init__()\n","        \n","        self.choice_param_name = ['alpha', 'beta', 'gamma']\n","        self.lifcal_param_name = ['tau', 'Vth', 'leak', 'conduct', 'reVth']\n","        self.T = lif_param['t']\n","        self.lif_param = lif_param\n","        self.tunable_lif = tunable_lif\n","        self.gamma = gamma\n","        self.beta = beta\n","        self.C_max = torch.Tensor([max_capacity])\n","        self.C_stop_iter = Capacity_max_iter\n","        self.loss_type = loss_type\n","        self.kld_weight = kld_weight_corrector\n","        self.latent_dim = latent_dim\n","\n","        image_channels = in_channels\n","        modules = []\n","        for h_dim in hidden_dims:\n","            modules.append(\n","                nn.Sequential(\n","                    layer.SeqToANNContainer(nn.Conv2d(in_channels, out_channels=h_dim, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm2d(h_dim)),\n","                    LIFSpike_CW(h_dim, **self.lif_param)\n","                    # nn.LeakyReLU())\n","            ))\n","            in_channels = h_dim\n","\n","        self.encoder = nn.Sequential(*modules)\n","        self.fc_mu = layer.SeqToANNContainer(nn.Linear(hidden_dims[-1]*4, latent_dim))\n","        self.fc_var = layer.SeqToANNContainer(nn.Linear(hidden_dims[-1]*4, latent_dim))\n","\n","        # Build Decoder\n","        modules = []\n","\n","        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*4)\n","\n","        hidden_dims.reverse()\n","\n","        for i in range(len(hidden_dims) - 1):\n","            modules.append(\n","                nn.Sequential(\n","                    layer.SeqToANNContainer(nn.ConvTranspose2d(hidden_dims[i],\n","                                       hidden_dims[i + 1],\n","                                       kernel_size=3,\n","                                       stride = 2,\n","                                       padding=1,\n","                                       output_padding=1),\n","                    nn.BatchNorm2d(hidden_dims[i + 1])),\n","                    LIFSpike_CW(hidden_dims[i + 1], **self.lif_param)\n","            ))\n","            \n","        self.decoder = nn.Sequential(*modules)\n","        self.final_layer = nn.Sequential(\n","                            layer.SeqToANNContainer(nn.ConvTranspose2d(hidden_dims[-1],\n","                                               hidden_dims[-1],\n","                                               kernel_size=3,\n","                                               stride=2,\n","                                               padding=1,\n","                                               output_padding=1),\n","                            nn.BatchNorm2d(hidden_dims[-1])),\n","                            LIFSpike_CW(hidden_dims[-1], **self.lif_param),\n","                            layer.SeqToANNContainer(nn.Conv2d(hidden_dims[-1], out_channels=image_channels,\n","                                      kernel_size= 3, padding= 1)),\n","                            nn.Tanh())\n","\n","        self._initialize_weights()\n","        print('steps:{}'.format(self.T),\n","              'init-tau:{}'.format(tau),\n","              'aa:{}'.format(aa),\n","              'Vth:{}'.format(Vth)\n","              )\n","\n","    def encode(self, input):\n","        input = input.unsqueeze(0).repeat(self.T, 1, 1, 1, 1)\n","        result = self.encoder(input)\n","        result = torch.flatten(result, start_dim=2)\n","        # Split the result into mu and var components\n","        # of the latent Gaussian distribution\n","        means = self.fc_mu(result)\n","        log_variances = self.fc_var(result)\n","        return [means, log_variances]\n","\n","    def decode(self, z):\n","        result = self.decoder_input(z)\n","        result = result.view(3, -1, 512, 2, 2)\n","        result = self.decoder(result)\n","        result = self.final_layer(result)\n","        return result\n","\n","    def forward(self, input):\n","        means, log_variances = self.encode(input)\n","        z = self.reparameterize(means, log_variances)\n","        return  [self.decode(z).mean(0), input, means.mean(0), log_variances.mean(0)]\n","    \n","    def reparameterize(self, means, log_variances):\n","        std = torch.exp(0.5 * log_variances)\n","        eps = torch.randn_like(std)\n","        return eps * std + means\n","\n","\n","    \n","    def loss_function(self, means, log_variances, output, target):\n","        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_variances - means ** 2 - log_variances.exp(), dim = 1), dim = 0)\n","        reconstruction_loss = F.mse_loss(output, target)\n","        if self.loss_type == \"beta\":\n","          loss = reconstruction_loss + self.beta * self.kld_weight * kld_loss\n","        else:\n","          self.C_max = self.C_max.to(device)\n","          C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n","          loss = reconstruction_loss + self.gamma * self.kld_weight* (kld_loss - C).abs()\n","        return {\"loss\":loss, \"Reconstruction_loss\": reconstruction_loss, \"KLD_loss\": kld_loss}\n","\n","    def generate(self, x,):\n","        \"\"\"\n","        Given an input image x, returns the reconstructed image\n","        :param x: (Tensor) [B x C x H x W]\n","        :return: (Tensor) [B x C x H x W]\n","        \"\"\"\n","        return self.forward(x)[0]\n","\n","    def sample(self, num_samples):\n","        \"\"\"\n","        Samples from the latent space and return the corresponding\n","        image space map.\n","        :param num_samples: (Int) Number of samples\n","        :param current_device: (Int) Device to run the model\n","        :return: (Tensor)\n","        \"\"\"\n","        z = torch.randn(num_samples,\n","                        self.latent_dim)\n","\n","        z = z.to(device)\n","\n","        samples = self.decode(z)\n","        return samples\n","    \n","    def randomize_gate(self):\n","        for name, m in self.named_modules():\n","            if all([hasattr(m, i) for i in self.choice_param_name]):\n","                for i in range(len(self.choice_param_name)):\n","                    setattr(m, self.choice_param_name[i],\n","                            nn.Parameter(\n","                                torch.tensor(init_constrain * (np.random.rand(m.plane) - 0.5)\n","                                             , dtype=torch.float)\n","                                        )\n","                            )\n","                    \n","\n","    def _initialize_weights(self):\n","        for name, m in self.named_modules():\n","            if isinstance(m, nn.Conv2d):\n","                if 'first' in name:\n","                    nn.init.normal_(m.weight, 0, 0.01)\n","                else:\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, tdBatchNorm)):\n","                if m.weight is not None:\n","                    nn.init.constant_(m.weight, 1)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","                nn.init.constant_(m.running_mean, 0)\n","            elif isinstance(m, nn.BatchNorm1d):\n","                nn.init.constant_(m.weight, 1)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0.0001)\n","                nn.init.constant_(m.running_mean, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":11103,"status":"error","timestamp":1680188104666,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"5WWGALejnJFe","outputId":"08c37f0e-5a4f-4c3e-aea6-3668cca0b723"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'gate': [0.6, 0.8, 0.6], 'param': [0.25, 0.5, 0.0625, 0.5], 't': 3, 'static_gate': False, 'static_param': False, 'time_wise': False, 'soft_mode': False}\n","steps:3 init-tau:0.25 aa:0.5 Vth:0.5\n","the random seed is 9\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e4c1f99e76c4238a308e77ad41b3bd2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10174 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-10-82dc6ff8f9fe\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 130\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 131\u001b[0;31m     loss_result = train(args, model, device, train_loader, optimizer, epochs, writer,\n\u001b[0m\u001b[1;32m    132\u001b[0m           scaler=scaler)\n\u001b[1;32m    133\u001b[0m     print('====\u003e Epoch: {} Loss: {}'.format(\n","\u001b[0;32m\u003cipython-input-7-097ba6f2b8c2\u003e\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch, writer, scaler)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 18\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_variances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-9-c89186ff9674\u003e\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 99\u001b[0;31m         \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_variances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-9-c89186ff9674\u003e\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 83\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Split the result into mu and var components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spikingjelly/clock_driven/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_seq)\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 877\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_to_ann_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spikingjelly/clock_driven/functional.py\u001b[0m in \u001b[0;36mseq_to_ann_forward\u001b[0;34m(x_seq, stateless_module)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstateless_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstateless_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 568\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstateless_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--\u003e 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["print(initial_dict)\n","#prepare the model\n","if args.fmnist:\n","    model = VanillaVAE_GLIF(lif_param=initial_dict, in_channels = 1)\n","\n","elif args.celeba:\n","    model = VanillaVAE_GLIF(lif_param=initial_dict, in_channels = 3, kld_weight_corrector=.00025)\n","\n","else: #cifar10\n","      model = VanillaVAE_GLIF(lif_param=initial_dict)\n","\n","\n","if args.randomgate:\n","    randomize_gate(model)\n","    # model.randomize_gate\n","    print('randomized gate')\n","\n","modeltag = args.modeltag\n","writer = SummaryWriter('./summaries/' + modeltag)\n","\n","dict_params = create_para_dict(args, model)\n","# recording the initial GLIF parameters\n","record_param(args, model, dict=dict_params, epoch=0, modeltag=modeltag)\n","# classify GLIF-related params\n","choice_param_name = ['alpha', 'beta', 'gamma']\n","lifcal_param_name = ['tau', 'Vth', 'leak', 'conduct', 'reVth']\n","all_params = model.parameters()\n","lif_params = []\n","lif_choice_params = []\n","lif_cal_params = []\n","\n","for pname, p in model.named_parameters():\n","    if pname.split('.')[-1] in choice_param_name:\n","        lif_params.append(p)\n","        lif_choice_params.append(p)\n","    elif pname.split('.')[-1] in lifcal_param_name:\n","        lif_params.append(p)\n","        lif_cal_params.append(p)\n","# fetch id\n","params_id = list(map(id, lif_params))\n","other_params = list(filter(lambda p: id(p) not in params_id, all_params))\n","# optimizer \u0026 scheduler\n","if args.tunable_lif:\n","    init_lr_diff = 10\n","    optimizer = torch.optim.SGD([\n","            {'params': other_params},\n","            {'params': lif_cal_params, \"weight_decay\": 0.},\n","            {'params': lif_choice_params, \"weight_decay\": 0., \"lr\":args.learning_rate / init_lr_diff}\n","        ],\n","            lr=args.learning_rate,\n","            momentum=0.9,\n","            weight_decay=args.weight_decay\n","        )\n","    scheduler = CosineAnnealingLR_Multi_Params_soft(optimizer,\n","                                                        T_max=[args.epochs, args.epochs, int(args.epochs)])\n","else:\n","    optimizer = torch.optim.SGD([\n","        {'params': other_params},\n","        {'params': lif_params, \"weight_decay\": 0.}\n","    ],\n","        lr=args.learning_rate,\n","        momentum=0.9,\n","        weight_decay=args.weight_decay\n","    )\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n","\n","# criterion = VAELoss(args)\n","device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n","#Distributed computation\n","# if torch.cuda.is_available():\n","#     loss_function = criterion.cuda()\n","# else:\n","#     loss_function = criterion.cpu()\n","\n","if args.auto_continue:\n","    lastest_model = get_model(modeltag)\n","    if lastest_model is not None:\n","        checkpoint = torch.load(lastest_model, map_location='cpu')\n","        epochs = checkpoint['epoch']\n","        if torch.cuda.device_count() \u003e 1:\n","            model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n","            checkpoint = deletStrmodule(checkpoint)\n","        model.load_state_dict(checkpoint['state_dict'], strict=True)\n","        print('load from checkpoint, the epoch is {}'.format(epochs))\n","        dict_params = read_param(epoch=epochs, modeltag=modeltag)\n","        for i in range(epochs):\n","            scheduler.step()\n","        epochs += 1\n","\n","\n","best = {'acc': 0., 'epoch': 0}\n","\n","if args.eval:\n","    lastest_model = get_model(modeltag, addr=args.eval_resume)\n","    if lastest_model is not None:\n","        epochs = -1\n","        checkpoint = torch.load(lastest_model, map_location='cpu')\n","        model.load_state_dict(checkpoint['state_dict'], strict=True)\n","        if torch.cuda.device_count() \u003e 1:\n","            device = torch.device(local_rank)\n","            model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n","            model = nn.parallel.DistributedDataParallel(model.cuda(), device_ids=[local_rank],\n","                                                        output_device=local_rank,\n","                                                        find_unused_parameters=False)\n","        else:\n","            model = model.to(device)\n","        test(args, model, device, val_loader, epochs, writer, criterion=criterion,\n","              modeltag=modeltag, best=best, dict_params=dict_params)\n","    else:\n","        print('no model detected')\n","    exit(0)\n","\n","\n","if torch.cuda.device_count() \u003e 1:\n","    device = torch.device(local_rank)\n","    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n","    model = nn.parallel.DistributedDataParallel(model.cuda(), device_ids=[local_rank], output_device=local_rank,\n","                                                find_unused_parameters=False)\n","else:\n","    model = model.to(device)\n","\n","\n","print('the random seed is {}'.format(args.seed))\n","\n","# amp\n","if args.amp:\n","    scaler = amp.GradScaler()\n","else:\n","    scaler = None\n","for t in range(args.epochs):\n","    loss_result = train(args, model, device, train_loader, optimizer, epochs, writer,\n","          scaler=scaler)\n","    print('====\u003e Epoch: {} Loss: {}'.format(\n","          t, loss_result))\n","    # if t % 1 == 0:\n","    #     test(args, model, device, val_loader, epochs, writer,\n","    #           modeltag=modeltag, best=best, dict_params=dict_params)\n","    # else:\n","    #     pass\n","    print('and lr now is {}'.format(scheduler.get_last_lr()))\n","    scheduler.step()\n","writer.close()\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":451,"status":"ok","timestamp":1680188109340,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"t1QtTt4OEFrA"},"outputs":[],"source":["# torch.save(model.cpu(), 'vae_glif_tunable.pt')\n","model = torch.load('vae_glif_sgd_3_epochs.pt').to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1680188104666,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"5PlLSpuOnuXo"},"outputs":[],"source":["test_input, test_label = next(iter((val_loader)))\n","test_input = test_input.to(device)\n","test_label = test_label.to(device)\n","\n","recons = model.generate(test_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1680188104666,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"mblUepuo6huX"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.imshow(test_input[0].cpu().detach().permute(1, 2, 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1680188104667,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"cSV8L2Hk6jsq"},"outputs":[],"source":["plt.imshow(recons[0].cpu().detach().permute(1, 2, 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1680188104667,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"nopQ19W2O0Iw"},"outputs":[],"source":["mu, log_var = model.encode(test_input)\n","mu[0][0] = mu[0][3] + 0.06\n","z = model.reparameterize(mu, log_var)\n","plt.imshow(model.decode(z).mean(0)[0].cpu().detach().permute(1, 2, 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1680188104667,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"Xo4qYrvO3Bpu"},"outputs":[],"source":["# new_perp = model.sample(1)\n","z = torch.randn(3, 1, 128)\n","z = z.to(device)\n","samples = model.decode(z).mean(0)\n","plt.imshow(  samples[0].cpu().detach().permute(1, 2, 0)  )"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680188114549,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"FaUYjXipBN2V"},"outputs":[],"source":["optimizer = torch.optim.SGD([\n","        {'params': other_params},\n","        {'params': lif_params, \"weight_decay\": 0.}\n","    ],\n","        lr=1e-3,\n","        momentum=0.9,\n","        weight_decay=0.\n","    )\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"elapsed":2918542,"status":"ok","timestamp":1680191035645,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"MB7GHQ1M0mAR","outputId":"5e047b16-ffe3-477d-e026-e4ec8fa5c71c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"877da938fd0248fbaae9c6ddc220be2a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10174 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["====\u003e Epoch: 0 Loss: {'loss': tensor(0.0289, device='cuda:0', grad_fn=\u003cAddBackward0\u003e), 'Reconstruction_loss': tensor(0.0247, device='cuda:0', grad_fn=\u003cMseLossBackward0\u003e), 'KLD_loss': tensor(4.2017, device='cuda:0', grad_fn=\u003cMeanBackward1\u003e)}\n","and lr now is [0.001, 0.001]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2c834b4d1ec47d0a035df001ced86f2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10174 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["====\u003e Epoch: 1 Loss: {'loss': tensor(0.0423, device='cuda:0', grad_fn=\u003cAddBackward0\u003e), 'Reconstruction_loss': tensor(0.0368, device='cuda:0', grad_fn=\u003cMseLossBackward0\u003e), 'KLD_loss': tensor(5.5110, device='cuda:0', grad_fn=\u003cMeanBackward1\u003e)}\n","and lr now is [0.00075, 0.00075]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe06ff352f6e4742a113fc71bbe41312","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10174 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["====\u003e Epoch: 2 Loss: {'loss': tensor(0.0235, device='cuda:0', grad_fn=\u003cAddBackward0\u003e), 'Reconstruction_loss': tensor(0.0185, device='cuda:0', grad_fn=\u003cMseLossBackward0\u003e), 'KLD_loss': tensor(4.9947, device='cuda:0', grad_fn=\u003cMeanBackward1\u003e)}\n","and lr now is [0.0002500000000000001, 0.0002500000000000001]\n"]}],"source":["if args.amp:\n","    scaler = amp.GradScaler()\n","else:\n","    scaler = None\n","for t in range(args.epochs):\n","    loss_result = train(args, model, device, train_loader, optimizer, epochs, writer,\n","          scaler=scaler)\n","    print('====\u003e Epoch: {} Loss: {}'.format(\n","          t, loss_result))\n","    # if t % 1 == 0:\n","    #     test(args, model, device, val_loader, epochs, writer,\n","    #           modeltag=modeltag, best=best, dict_params=dict_params)\n","    # else:\n","    #     pass\n","    print('and lr now is {}'.format(scheduler.get_last_lr()))\n","    scheduler.step()\n","writer.close()"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1680191050010,"user":{"displayName":"Sonal Bihani","userId":"05648069857976220488"},"user_tz":240},"id":"byDX2UWQAWd9"},"outputs":[],"source":["torch.save(model.cpu(), 'vae_glif_sgd_6_epochs.pt')\n","# model = torch.load('vae_glif_sgd_3_epochs.pt').to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tk_tWmeMMou"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMBKGrQuMBX68KT93TU+eX0","mount_file_id":"1c88OiNLOjIFyZ6LIDWQvIL0txqMBUzXE","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"080d4a8c947a41ceb24c81f28b65b190":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a694d0dc53e47eca474dbc920427547":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fec6063b2b324c9996eacbc783664788","placeholder":"​","style":"IPY_MODEL_4274662c6a0e4f4b980e4ad6e19efc7b","value":"100%"}},"0c083254bd804d628e9d31b5338c2fb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f6cabae4a3f4f31b2068a4711fb69dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ae1b7e3be61481793e0725a44141c9a","placeholder":"​","style":"IPY_MODEL_f3665a3bf2ff4e60ae0b1eef3ed0c2dc","value":"100%"}},"1ae1b7e3be61481793e0725a44141c9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"205581628e9c4727af4b98d5dd76efb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2566b02fda8b4435bd05ba123f077f74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c1b47c6790b4a24a722aa8941debe5b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32339332a75d4d2bba471d63ab892a58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c1b47c6790b4a24a722aa8941debe5b","placeholder":"​","style":"IPY_MODEL_205581628e9c4727af4b98d5dd76efb8","value":"  0%"}},"32f681ba0029433284764d1c276c17ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e4c1f99e76c4238a308e77ad41b3bd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32339332a75d4d2bba471d63ab892a58","IPY_MODEL_ef6af2b43fcd473ab4bacb967ee1a7e3","IPY_MODEL_457de67266104619b8fc2253f867877f"],"layout":"IPY_MODEL_080d4a8c947a41ceb24c81f28b65b190"}},"4274662c6a0e4f4b980e4ad6e19efc7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"457de67266104619b8fc2253f867877f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e618569eddd4175a843a06ceaa1cd37","placeholder":"​","style":"IPY_MODEL_2566b02fda8b4435bd05ba123f077f74","value":" 0/10174 [00:06\u0026lt;?, ?it/s]"}},"4c76e6de693e495393c7686c6a4c3549":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32f681ba0029433284764d1c276c17ff","max":10174,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e14f57d2da134fd1b17c19b82e0c27a4","value":10174}},"4ea6016a28644c138fb58b2b21835eb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52ffc18800894570b1dff7ef12a314db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e618569eddd4175a843a06ceaa1cd37":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"731bd67b84824497849cc1ec7de69d1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"767fc995189142dcb11e61c4babf5636":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7893a035f21b4710a4a0ae608349b047":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_767fc995189142dcb11e61c4babf5636","placeholder":"​","style":"IPY_MODEL_864435bf334e4190a3a5c9397a13612f","value":" 10174/10174 [16:17\u0026lt;00:00, 13.25it/s]"}},"794cf696a28e43df853c379c6956e8d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd8e634a13b149449e81ba465eb5c065","max":10174,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9779b9e36ad946bd88a775f82b70fcdd","value":10174}},"8102f01030d9488fb6a00bfdd9da8501":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85695f7000f745dda251d00aafea57e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"864435bf334e4190a3a5c9397a13612f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"877da938fd0248fbaae9c6ddc220be2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a694d0dc53e47eca474dbc920427547","IPY_MODEL_d82f1c231c6b4c049dd7294c21123b3f","IPY_MODEL_7893a035f21b4710a4a0ae608349b047"],"layout":"IPY_MODEL_92d3c81cad0c4db8a68a9ad526e0153a"}},"92d3c81cad0c4db8a68a9ad526e0153a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9779b9e36ad946bd88a775f82b70fcdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ff174f84e4a4c46947a679533e885f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b040976c99a84c84af157d3cae975880":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2dd2ad57500490f8666fcd76a9373a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_731bd67b84824497849cc1ec7de69d1b","placeholder":"​","style":"IPY_MODEL_8102f01030d9488fb6a00bfdd9da8501","value":" 10174/10174 [16:14\u0026lt;00:00, 11.29it/s]"}},"bb726cce2d1448cea0f5b0204dcca6c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c16c1a2c306c4e4a8889d6ac8108db05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1d656bba96248a98d5ab83c7c3975e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7b24d3091484b79ab3e4bb8e787cf39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85695f7000f745dda251d00aafea57e0","placeholder":"​","style":"IPY_MODEL_b040976c99a84c84af157d3cae975880","value":" 10174/10174 [16:05\u0026lt;00:00, 13.28it/s]"}},"d82f1c231c6b4c049dd7294c21123b3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52ffc18800894570b1dff7ef12a314db","max":10174,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c083254bd804d628e9d31b5338c2fb5","value":10174}},"e14f57d2da134fd1b17c19b82e0c27a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2525143110c44a992733ba428098aa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ea6016a28644c138fb58b2b21835eb7","placeholder":"​","style":"IPY_MODEL_9ff174f84e4a4c46947a679533e885f4","value":"100%"}},"ef6af2b43fcd473ab4bacb967ee1a7e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb726cce2d1448cea0f5b0204dcca6c3","max":10174,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1d656bba96248a98d5ab83c7c3975e8","value":0}},"f2c834b4d1ec47d0a035df001ced86f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2525143110c44a992733ba428098aa5","IPY_MODEL_794cf696a28e43df853c379c6956e8d5","IPY_MODEL_d7b24d3091484b79ab3e4bb8e787cf39"],"layout":"IPY_MODEL_f73921bc13374bfd81cc121fa7d1cf40"}},"f3665a3bf2ff4e60ae0b1eef3ed0c2dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f73921bc13374bfd81cc121fa7d1cf40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd8e634a13b149449e81ba465eb5c065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe06ff352f6e4742a113fc71bbe41312":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f6cabae4a3f4f31b2068a4711fb69dd","IPY_MODEL_4c76e6de693e495393c7686c6a4c3549","IPY_MODEL_b2dd2ad57500490f8666fcd76a9373a5"],"layout":"IPY_MODEL_c16c1a2c306c4e4a8889d6ac8108db05"}},"fec6063b2b324c9996eacbc783664788":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}